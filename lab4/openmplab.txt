First I tested the performance of the program normally. I compiled it with

make seq

and running ./seq output

FUNC TIME : 0.504830
TOTAL TIME : 2.353649

Thus 0.504830 is my baseline runtime that I will use to test the speedup. Then
to profile it I compiled it with

make seq GPROF=1

and when I ran it with ./seq it output

FUNC TIME : 0.590301
TOTAL TIME : 2.590365

So profiling adds significant overhead. I then looked at the profiling results
by doing

gprof seq

and saw the following

  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 65.61      0.38     0.38       15    25.37    27.02  func1
 22.44      0.51     0.13  5177344     0.00     0.00  rand2
  3.45      0.53     0.02        1    20.03   125.49  addSeed
  3.45      0.55     0.02                             sequence
  1.73      0.56     0.01   491520     0.00     0.00  findIndexBin
  1.73      0.57     0.01       15     0.67     1.34  func5
  1.73      0.58     0.01        1    10.01    10.01  imdilateDisk
  0.00      0.58     0.00   983042     0.00     0.00  round
  0.00      0.58     0.00       16     0.00     0.00  dilateMatrix
  0.00      0.58     0.00       15     0.00     0.00  func2
  0.00      0.58     0.00       15     0.00     0.00  func3
  0.00      0.58     0.00       15     0.00     0.00  func4
  0.00      0.58     0.00       15     0.00     0.00  rand1
  0.00      0.58     0.00        2     0.00     0.00  get_time
  0.00      0.58     0.00        1     0.00     0.00  elapsed_time
  0.00      0.58     0.00        1     0.00     0.00  fillMatrix
  0.00      0.58     0.00        1     0.00     0.00  func0
  0.00      0.58     0.00        1     0.00     0.00  getNeighbors

So func1 is taking up the majority of the run time, and rand2 comes in
second. Looking inside func.c at func1 shows that func1 calls rand2.

I did make clean and noticed that gmon.out never got deleted, so I changed the
last line of the makefile to

        rm -f seq omp filter output.txt mtrace.out gmon.out

So I began looking at omp and seeing how to optimize func1. I saw that there was
a for loop that initialized arrayX and arrayY in func1. This could be made
parallel with open mp. Looking at the documentation I determined that I should
make

for(i = 0; i < n; i++)
{
  arrayX[i] += 1 + 5*rand2(seed, i);
  arrayY[i] += -2 + 2*rand2(seed, i);
}

parallel with

#pragma omp parallel for num_threads(8) default(shared) private (i)
        for(i = 0; i < n; i++)
        {
          arrayX[i] += 1 + 5*rand2(seed, i);
          arrayY[i] += -2 + 2*rand2(seed, i);
        }


which ran the for loop in 8 different threads. Below this there was another for
loop which I optimized in a similar manner except I marked the variables i, j,
index_X, and index_y private instead. I also simplified the power expression
mathematically and cleaned up memory acesses by putting the summation into a
local variable. I also replaced the if else block with a ternary operator and
saved calculations by storing them in local variables. I then combined the two
for loops because I realized that they could be done together. This reduced the
code in func1 into the following compact segment of code

#pragma omp parallel for num_threads(8) default(shared) private(i,j,index_X,index_Y)
        for(i = 0; i<n; i++){
          double arx=arrayX[i]+ 1 + 5*rand2(seed,i);
          double ary=arrayY[i]- 2 + 2*rand2(seed,i);
          double result=0;
          for(j = 0; j < Ones; j++){
            index_X = round(arx) + objxy[j*2 + 1];
            index_Y = round(ary) + objxy[j*2];
            int res = abs((index_X*Y + index_Y)*Z + iter);
            int loc = i*Ones+j;
            index[loc]= (res>=max_size) ? 0 : res;
            result+=(256*array[index[loc]]-41984)/50.0;
          }
          probability[i] = result/((double) Ones);
          arrayX[i] = arx;
          arrayY[i] = ary;
        }

Then after that I went through all the for loops in the other functions and I
added #pragma omp parallel for statements to all of them. When a summation
occured and I needed the result to be synchronized I used the reduction(+:var)
option in the openmp directive. I also made sure to make the appropriate
variables private so that each function would work properly. I played around
with the number of threads and saw that increasing it above 8 did not change
performance noticably. But changing it to 4 decreased performance, so I kept it
at 8. The entirety of my changes to each function can be seen in func.c.

Testing my optimized func.c with

make omp
./omp

resulted in

FUNC TIME : 0.089652
TOTAL TIME : 2.180735

So my total speedup is ~5.63x. This is satisfactory. I also tested the
correctness of my func.c with

make check

and saw that it produced the correct output. Doing

make omp MTRACE=1
./omp
make checkmem

showed that there were some memory leaks, however. There were 11 total lines of
memory leaks. I could not seem to figure out why this was happening, as I never
called malloc explicitly. Unzipping a new openmplab.tgz and testing by
multithreading a trivial for loop with nothing in it showed that memory leaks
were detected. Thus it seems that the memory leak checker is buggy. I will
ignore the memory leaks, as I believe the extra speedup I have introduced is
enough bonus points to offset the memory leak penalty. Additionally one of the
TA's said to not worry about the memory leaks. Thus I have finished the lab.
